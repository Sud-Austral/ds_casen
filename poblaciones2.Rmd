---
title: "Análisis de la anomalía en la Casen 2015"
author: "Víctor Enamorado-Christian Castro. DI"
date: "15-12-2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
library(ggpubr)
library(markdown)
library(shiny)
library(shinythemes)
library(tidyverse)
library(magrittr)
library(lubridate)
library(plotly)
library(xts)
library(dygraphs)
library(kableExtra)
library(knitr)
library("readxl")
library(rsconnect)
library(dplyr)
library(summarytools)
library(epiDisplay)
#library(leaflet)
library(haven)
library(epiDisplay)
library("readxl")
library(expss)
library(hrbrthemes)
library(viridis)
library(viridisLite)
library(DescTools)
library(roperators)
library(shinycssloaders)
library(writexl)
library(vroom)
library(shinyWidgets)
library(stringr)
library(dplyr)
library(gapminder)
library(tidyverse)
library(moderndive)
library(skimr)

oldw <- getOption("warn")
options(warn = -1)


# Códigos Únicos Territoriales:
# http://www.subdere.gov.cl/documentacion/c%C3%B3digos-%C3%BAnicos-territoriales-actualizados-al-06-de-septiembre-2018

```

#### Tomaremos la proyección de la población establecida por el INE con base en el 2017 para los años 2006, 2009, 2011, 2013, 2015 y 2017 y la compararemos con la suma total de las frecuencias expandidas de las tablas de contingencia generadas en torno a las variables de sexo y pobreza (Corte) a nivel comunal.

Para el 2015 se detecta una anomalía.

Fuentes de datos:

Proyección base 2017
Estimaciones y proyecciones 2002-2035, comunas: \
https://www.ine.cl/estadisticas/sociales/demografia-y-vitales/proyecciones-de-poblacion

#### Leemos la base de datos referida a población del INE desde un csv, la transformamos a rds y la cargamos:

```{r}
poblacion_chilena <- read.csv2('poblacion_chilena.csv', stringsAsFactors=FALSE)
saveRDS(poblacion_chilena, file = "poblacion_chilena.rds")
pob_chilena  <- readRDS("poblacion_chilena.rds")
```
Visualicemos sus cuatro primeras líneas:

```{r}

head(pob_chilena,4)

```

Ésta información la tenemos desglosada por sexo y edad, pero la queremos simplemente agregada a nivel de comuna y año. 
```{r}
pob_por_comuna_ine_2006 <- aggregate(pob_chilena$Poblacion.2006, by=list(Comuna=pob_chilena$Nombre.Comuna), FUN=sum)
pob_por_comuna_ine_2009 <- aggregate(pob_chilena$Poblacion.2009, by=list(Comuna=pob_chilena$Nombre.Comuna), FUN=sum)
pob_por_comuna_ine_2011 <- aggregate(pob_chilena$Poblacion.2011, by=list(Comuna=pob_chilena$Nombre.Comuna), FUN=sum)
pob_por_comuna_ine_2013 <- aggregate(pob_chilena$Poblacion.2013, by=list(Comuna=pob_chilena$Nombre.Comuna), FUN=sum)
pob_por_comuna_ine_2015 <- aggregate(pob_chilena$Poblacion.2015, by=list(Comuna=pob_chilena$Nombre.Comuna), FUN=sum)
pob_por_comuna_ine_2017 <- aggregate(pob_chilena$Poblacion.2017, by=list(Comuna=pob_chilena$Nombre.Comuna), FUN=sum)


```
Despleguemos la población para las primeras 4 comunas del dataset para el 2017:


```{r}
head(pob_por_comuna_ine_2017,4)
```

Una vez listos estos datasets, vamos a construir tablas de contingencia relativas a la clasificación de las personas según su situación de pobreza y sexo y a comparar las sumas totales de estos resultados (que ya están expandidos) con el de la población total comunal. Deben aproximarse.

#### Digresión

Actualmente la poblacion esta disminuyendo?

```{r}

sum(pob_por_comuna_ine_2006[,2])
sum(pob_por_comuna_ine_2009[,2])
sum(pob_por_comuna_ine_2011[,2])
sum(pob_por_comuna_ine_2013[,2])
sum(pob_por_comuna_ine_2015[,2])
sum(pob_por_comuna_ine_2017[,2])

```

En general no, pero Canela y Carahue si, no es absurdo pensar que disminuye en algunas actualmente.



<br>

###########################################################
## Análisis para el año 2015
###########################################################

Al comparar las sumas de las frecuencias de las tablas de contingencia con el valor de la población para cada comuna, observamos que no coinciden y por mucho, situación que es evidente cuando se compara con el total de la población nacional, donde el descuadre es de varios millones.

Sospechamos que existen comunas que no poseen factor de expansión en la Casen 2015

```{r}
dataset2015  <- readRDS("dataset2015.rds")
sum(is.na(dataset2015$expc))
```

Así es: 52770 no tienen factor de expansión comunal. ¿Que comunas son?

```{r}
new_DF <- subset(dataset2015, is.na(dataset2015$expc))
newdata <- new_DF[c("comuna", "expc")]
newdata <- distinct(newdata, comuna, .keep_all = TRUE)
write.csv(newdata,"comunas_sin_expc_2015.csv", row.names = FALSE)
# head(newdata, 10)

```
<span style="color:red">*1 Son 185!*</span>

<span style="color:red">*2 El problema es que los factores de expansión están asociados a cada una de LAS OBSERVACIONES, por lo que no se puede obtener un factor de expansión comunal que sea extrapolable a otra base de datos.*</span>


Estrategia

Es entonces que las promediaremos por comuna para el 2013 y el 2017 y las promediaremos, redondeando al entero.

Haremso un subset con la data del 2015 que no tiene fdee y le asociaremos uno como ésta media. Luego lo uniremos al subset remanente del dataset2015.

Obtengamos los promedios del fde comunal para el 2013:

#### 2013


```{r}

dataset2013  <- readRDS("dataset2013.rds")

dataset2013_sub <- dataset2013[  , c("folio", "comuna", "expc")]
    
dataset2013_sub_expc_prom <- aggregate(dataset2013_sub$expc, by=list(Comuna=dataset2013_sub$comuna), FUN=mean)

dataset2013_sub_expc_prom$`mean.dataset2013_sub$expc`<-round(dataset2013_sub_expc_prom$`mean.dataset2013_sub$expc`)

head(dataset2013_sub_expc_prom, 10)
```






Les asociamos el código comunal correcto:

```{r}

data_cod <- dataset2013[  , c("folio", "comuna")]
    
names(data_cod)[2] <- "Comuna"
data_cod <- distinct(data_cod ,Comuna, .keep_all = TRUE)
data_cod <- data_cod %>% mutate(Comuna = str_squish(Comuna))
data_cod <- data_cod %>% mutate(codigo = case_when(as.integer(folio / 10000000000) == 0 ~ as.integer(folio/ 10000000), as.integer(folio / 10000000000) <17 ~ as.integer(folio / 10000000)))
    
    data_cod <- subset( data_cod, select = -folio )
    data_cod[171,2]<-16101
    data_cod[172,2]<-16102
    data_cod[173,2]<-16202
    data_cod[174,2]<-16203
    data_cod[175,2]<-16302
    data_cod[176,2]<-16103
    data_cod[177,2]<-16104
    data_cod[178,2]<-16204
    data_cod[179,2]<-16303
    data_cod[180,2]<-16105
    data_cod[181,2]<-16106
    data_cod[182,2]<-16205
    data_cod[183,2]<-16107
    data_cod[184,2]<-16201
    data_cod[185,2]<-16206
    data_cod[186,2]<-16301
    data_cod[187,2]<-16304
    data_cod[188,2]<-16108
    data_cod[189,2]<-16305
    data_cod[190,2]<-16207
    data_cod[191,2]<-16109
    

    df_2013 = merge( x = dataset2013_sub_expc_prom, y = data_cod, by = "Comuna", all.x = TRUE)
    
   head(df_2013,10)
```

#### Hacemos lo mismo para el 2017


```{r}

dataset2017  <- readRDS("dataset2017.rds")

dataset2017_sub <- dataset2017[ , c("folio", "comuna", "expc")]
    
dataset2017_sub_expc_prom <- aggregate(dataset2017_sub$expc, by=list(Comuna=dataset2017_sub$comuna), FUN=mean)

dataset2017_sub_expc_prom$`mean.dataset2017_sub$expc`<-round(dataset2017_sub_expc_prom$`mean.dataset2017_sub$expc`)

head(dataset2017_sub_expc_prom, 10)
```

Les asociamos el código comunal correcto:

```{r}

      
      data_code <- dataset2017[  , c("folio", "comuna")]
      
      names(data_code)[2] <- "Comuna"
      data_code <- distinct(data_code , Comuna, .keep_all = TRUE)
      #
      data_code <- data_code %>% mutate(Comuna = str_squish(Comuna))
      
      data_code <- data_code %>% mutate(codigo = case_when(as.integer(folio / 1000000000000) == 0 ~ as.integer(folio/ 100000000)
                                                           , as.integer(folio / 1000000000000) <17 ~ as.integer(folio / 100000000)
                                                           
      ))
      
      data_cod <- subset( data_code, select = -folio )
      
      data_cod[172,2]<-16102
      data_cod[171,2]<-16101
      data_cod[176,2]<-16103
      data_cod[173,2]<-16202
      data_cod[174,2]<-16203
      data_cod[175,2]<-16302
      data_cod[177,2]<-16104
      data_cod[178,2]<-16204
      data_cod[179,2]<-16303
      data_cod[180,2]<-16105
      data_cod[181,2]<-16106
      data_cod[182,2]<-16205
      data_cod[183,2]<-16107
      data_cod[184,2]<-16201
      data_cod[185,2]<-16206
      data_cod[186,2]<-16301
      data_cod[187,2]<-16304
      data_cod[188,2]<-16108
      data_cod[189,2]<-16305
      data_cod[190,2]<-16207
      data_cod[191,2]<-16109
      

      
      df_2017 = merge( x = dataset2017_sub_expc_prom, y = data_cod, by = "Comuna", all.x = TRUE)

      
      
head(df_2017, 10)
```

Ahora unimos los datasets y calculamos un promedio para el f de e:

```{r}
factores_de_exp_mean <- merge( x = df_2013, y = df_2017, by = "codigo", all.x = TRUE)
factores_de_exp_mean$mean <- rowMeans(factores_de_exp_mean[,c('mean.dataset2013_sub$expc', 'mean.dataset2017_sub$expc')], na.rm=TRUE)
factores_de_exp_mean$mean <- round(factores_de_exp_mean$mean)
head(factores_de_exp_mean, 10)
```



### Aplicación al 2015 

Ahora debemos asignar el mismo factor de expansión de acuerdo a las comunas a las que pertenecen cada uno de las observaciones que en la Casen 2015 no lo tienen.


La estrategia es construir una subset de la data que no contiene el factor de expansion, para luego agregarla a la data 2015 que si los tiene y asi construir un dataset completo.

Tomamos factores_de_exp_mean y hacemos un merge por codigo con la Casen2015 original

Pero primero obtengamos el dataset2015 con sus códigos comunales correctos:

```{r}
dataset2015  <- readRDS("dataset2015.rds")

      data_code <- dataset2015[  , c("folio", "comuna")]
      
      data_code <- distinct(data_code , comuna, .keep_all = TRUE)
      
      data_code <- data_code %>% mutate(comuna = str_squish(comuna))
      
      data_code <- data_code %>% mutate(codigo = case_when(as.integer(folio / 10000000000) == 0 ~ as.integer(folio/ 10000000)
                                                           , as.integer(folio / 10000000000) <17 ~ as.integer(folio / 10000000)))
      
      data_cod <- subset( data_code, select = -folio )
      data_cod[171,2]<-16101
      data_cod[172,2]<-16102
      data_cod[173,2]<-16202
      data_cod[174,2]<-16203
      data_cod[175,2]<-16302
      data_cod[176,2]<-16103
      data_cod[177,2]<-16104
      data_cod[178,2]<-16204
      data_cod[179,2]<-16303
      data_cod[180,2]<-16105
      data_cod[181,2]<-16106
      data_cod[182,2]<-16205
      data_cod[183,2]<-16107
      data_cod[184,2]<-16201
      data_cod[185,2]<-16206
      data_cod[186,2]<-16301
      data_cod[187,2]<-16304
      data_cod[188,2]<-16108
      data_cod[189,2]<-16305
      data_cod[190,2]<-16207
      data_cod[191,2]<-16109

      df_2015 = merge( x = dataset2015, y = data_cod, by = "comuna", all.x = TRUE)

# head(df_2015, 10)


```


El problema es que tenemos todas las comunas. Debemos hacer un subset solo con las de interes!

```{r}
df_2015_sub <- subset(df_2015, is.na(df_2015$expc))
# head(df_2015_sub, 10)

```

Como uno de los pasos finales unimos nuestra tabla con los promedios del f de e comunal, con el subset anterior:


```{r}
db_final_2015 = merge( x = df_2015_sub, y = factores_de_exp_mean, by = "codigo", all.x = TRUE)
# head(db_final_2015,10)

```

Verificamos que posee el mismo numero de filas:

```{r}
nrow(db_final_2015)
```

Y verificamos que todas sus filas posean el factor de expansión que hemos construído y asignado:


```{r}
sum(is.na(db_final_2015$mean))
```

Ahora sólo hay que ordenar la cantidad y nombres de las columnas, eliminar los na expc del dataset2015 y añadir nuestro ultimo dataframe al final.

Así quedaría lista la Casen2015 para ser trabajada.







































